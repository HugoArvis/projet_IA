#!/usr/bin/env python3
# ============================================================================
# RUN_TUNING.PY - Script pour ex√©cuter uniquement le fine-tuning
# ============================================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
import os

from hyperparameter_tuning import run_full_tuning


def main():
    """
    Script principal pour le fine-tuning des mod√®les
    """

    print("=" * 80)
    print("SCRIPT DE FINE-TUNING DES MOD√àLES")
    print("=" * 80)

    # Charger les donn√©es
    print("\nüìÇ Chargement des donn√©es...")
    data = pd.read_csv('data/prepared_data.csv')

    X = data.drop('Attrition', axis=1)
    y = data['Attrition']

    print(f"‚úì Donn√©es charg√©es: {X.shape[0]} lignes, {X.shape[1]} colonnes")

    # Split train/test
    print("\nüìä S√©paration train/test...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"‚úì Train set: {len(X_train)} exemples")
    print(f"‚úì Test set: {len(X_test)} exemples")

    # Normalisation
    print("\nüîÑ Normalisation des donn√©es...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

    print("‚úì Normalisation termin√©e")

    # Sauvegarder le scaler
    os.makedirs('tuned_models', exist_ok=True)
    joblib.dump(scaler, 'tuned_models/scaler.pkl')
    print("üíæ Scaler sauvegard√©: tuned_models/scaler.pkl")

    # Menu de configuration
    print("\n" + "=" * 80)
    print("CONFIGURATION DU FINE-TUNING")
    print("=" * 80)

    print("\nüîß Choisissez le type de recherche:")
    print("   1. Grid Search (exhaustif, LENT mais pr√©cis)")
    print("   2. Random Search (√©chantillonnage al√©atoire, RAPIDE)")

    search_choice = input("\nVotre choix (1 ou 2): ")
    search_type = 'grid' if search_choice == '1' else 'random'

    print("\nüìä Choisissez la taille de la grille:")
    print("   1. Grille r√©duite (rapide, ~5-10 min)")
    print("   2. Grille compl√®te (lent, ~20-60 min)")

    grid_choice = input("\nVotre choix (1 ou 2): ")
    small_grid = (grid_choice == '1')

    print("\n‚öñÔ∏è  Utiliser SMOTE pour √©quilibrer les classes?")
    print("   1. Oui (RECOMMAND√â pour classes d√©s√©quilibr√©es)")
    print("   2. Non")

    smote_choice = input("\nVotre choix (1 ou 2): ")
    use_smote = (smote_choice == '1')

    # R√©sum√© de la configuration
    print("\n" + "=" * 80)
    print("R√âSUM√â DE LA CONFIGURATION")
    print("=" * 80)
    print(f"‚úì Type de recherche: {search_type.upper()}")
    print(f"‚úì Taille de grille: {'R√©duite' if small_grid else 'Compl√®te'}")
    print(f"‚úì SMOTE: {'Activ√©' if use_smote else 'D√©sactiv√©'}")
    print(f"‚úì Validation crois√©e: 5 folds")

    # Estimation du temps
    if search_type == 'grid':
        time_estimate = "5-10 minutes" if small_grid else "20-60 minutes"
    else:
        time_estimate = "3-7 minutes" if small_grid else "10-30 minutes"

    print(f"\n‚è±Ô∏è  Temps estim√©: {time_estimate}")

    input("\nAppuyez sur Entr√©e pour lancer le fine-tuning...")

    # Lancer le fine-tuning
    print("\n" + "=" * 80)
    print("D√âMARRAGE DU FINE-TUNING")
    print("=" * 80)

    tuned_models, best_params, results = run_full_tuning(
        X_train_scaled, X_test_scaled, y_train, y_test,
        use_smote=use_smote,
        search_type=search_type,
        small_grid=small_grid
    )

    # Afficher les instructions finales
    print("\n" + "=" * 80)
    print("üìÅ FICHIERS G√âN√âR√âS")
    print("=" * 80)
    print("\nLes fichiers suivants ont √©t√© cr√©√©s dans le dossier 'tuned_models/':")
    print("\nüîπ Mod√®les optimis√©s:")
    print("   ‚Ä¢ logistic_regression_tuned.pkl")
    print("   ‚Ä¢ svm_(linear)_tuned.pkl")
    print("   ‚Ä¢ random_forest_tuned.pkl")
    print("   ‚Ä¢ xgboost_tuned.pkl")

    print("\nüîπ R√©sultats et analyses:")
    print("   ‚Ä¢ best_hyperparameters.csv - Meilleurs hyperparam√®tres")
    print("   ‚Ä¢ final_test_results.csv - R√©sultats sur test set")
    print("   ‚Ä¢ *_cv_results.csv - R√©sultats d√©taill√©s de la CV")
    print("   ‚Ä¢ *_tuning_analysis.png - Graphiques d'analyse")

    print("\nüîπ Utilitaires:")
    print("   ‚Ä¢ scaler.pkl - Objet de normalisation")

    print("\n" + "=" * 80)
    print("üí° COMMENT UTILISER LES MOD√àLES OPTIMIS√âS")
    print("=" * 80)

    print("""
import joblib
import pandas as pd

# 1. Charger le scaler
scaler = joblib.load('tuned_models/scaler.pkl')

# 2. Charger le meilleur mod√®le (exemple: Random Forest)
model = joblib.load('tuned_models/random_forest_tuned.pkl')

# 3. Pr√©parer vos nouvelles donn√©es
# new_data = pd.read_csv('new_employees.csv')
# new_data_scaled = scaler.transform(new_data)

# 4. Faire des pr√©dictions
# predictions = model.predict(new_data_scaled)
# probabilities = model.predict_proba(new_data_scaled)[:, 1]

# 5. Identifier les employ√©s √† risque
# at_risk = new_data[predictions == 1]
# print(f"Employ√©s √† risque: {len(at_risk)}")
    """)

    print("\n" + "=" * 80)
    print("‚úÖ FINE-TUNING TERMIN√â AVEC SUCC√àS!")
    print("=" * 80)

    # Afficher le meilleur mod√®le
    best_model_row = results.iloc[0]
    print(f"\nüèÜ Meilleur mod√®le sur le test set:")
    print(f"   Mod√®le: {best_model_row['Mod√®le']}")
    print(f"   F1-Score: {best_model_row['F1-Score']:.4f}")
    print(f"   Precision: {best_model_row['Precision']:.4f}")
    print(f"   Recall: {best_model_row['Recall']:.4f}")
    print(f"   ROC-AUC: {best_model_row['ROC-AUC']:.4f}")


if __name__ == "__main__":
    main()