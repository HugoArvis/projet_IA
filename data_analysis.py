import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats


def analyze_data(filepath='data/prepared_data.csv'):
    """
    Effectue une analyse exploratoire complÃ¨te des donnÃ©es
    """
    print("=" * 70)
    print("ANALYSE EXPLORATOIRE DES DONNÃ‰ES")
    print("=" * 70)

    # Charger les donnÃ©es
    data = pd.read_csv(filepath)

    # Informations gÃ©nÃ©rales
    print(f"\nğŸ“Š Forme du dataset: {data.shape}")
    print(f"   - Nombre de lignes: {data.shape[0]}")
    print(f"   - Nombre de colonnes: {data.shape[1]}")

    # Distribution de la target
    print("\nğŸ¯ Distribution de la variable cible (Attrition):")
    attrition_counts = data['Attrition'].value_counts()
    for label, count in attrition_counts.items():
        percentage = count / len(data) * 100
        label_name = "Pas d'attrition" if label == 0 else "Attrition"
        print(f"   - {label_name}: {count} ({percentage:.1f}%)")

    # Ratio de dÃ©sÃ©quilibre
    ratio = attrition_counts[0] / attrition_counts[1]
    print(f"   - Ratio de dÃ©sÃ©quilibre: {ratio:.2f}:1")

    if ratio > 3:
        print("   âš ï¸  ATTENTION: DÃ©sÃ©quilibre important dÃ©tectÃ©!")
        print("   â†’ Recommandation: Utiliser SMOTE/SMOTEENN + class_weight='balanced'")

    # Statistiques descriptives
    print("\nğŸ“ˆ Statistiques descriptives (features numÃ©riques):")
    X = data.drop('Attrition', axis=1)
    print(X.describe().T[['mean', 'std', 'min', 'max']].head(10))

    # Valeurs manquantes
    print("\nğŸ” Valeurs manquantes:")
    missing = data.isnull().sum()
    if missing.sum() > 0:
        print(missing[missing > 0])
    else:
        print("   âœ“ Aucune valeur manquante")

    # CorrÃ©lations avec la target
    print("\nğŸ”— CorrÃ©lations avec Attrition (Top 10):")
    correlations = X.corrwith(data['Attrition']).abs().sort_values(ascending=False)
    print(correlations.head(10))

    # Features potentiellement problÃ©matiques
    print("\nâš ï¸  Features avec variance trÃ¨s faible:")
    low_variance = X.var().sort_values().head(5)
    print(low_variance)

    # Identifier les features binaires
    print("\nğŸ”¢ Features binaires dÃ©tectÃ©es:")
    binary_features = [col for col in X.columns if X[col].nunique() == 2]
    print(f"   {len(binary_features)} features binaires trouvÃ©es")
    print(f"   Exemples: {binary_features[:5]}")

    return data, correlations


def plot_top_correlations(data, n=10):
    """
    Visualise les features les plus corrÃ©lÃ©es avec Attrition
    """
    X = data.drop('Attrition', axis=1)
    y = data['Attrition']

    # Calculer corrÃ©lations
    correlations = X.corrwith(y).sort_values(ascending=False)
    top_positive = correlations.head(n // 2)
    top_negative = correlations.tail(n // 2)
    top_features = pd.concat([top_positive, top_negative])

    # Plot
    plt.figure(figsize=(10, 8))
    colors = ['green' if x > 0 else 'red' for x in top_features.values]
    plt.barh(range(len(top_features)), top_features.values, color=colors, alpha=0.7)
    plt.yticks(range(len(top_features)), top_features.index)
    plt.xlabel('CorrÃ©lation avec Attrition', fontsize=12)
    plt.title(f'Top {n} Features corrÃ©lÃ©es avec l\'Attrition', fontsize=14, fontweight='bold')
    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
    plt.tight_layout()
    plt.savefig('trained_models/correlation_analysis.png', dpi=300, bbox_inches='tight')
    print("\nğŸ“Š Graphique sauvegardÃ©: trained_models/correlation_analysis.png")
    plt.show()


def check_data_quality(data):
    """
    VÃ©rifie la qualitÃ© des donnÃ©es et identifie les problÃ¨mes potentiels
    """
    print("\n" + "=" * 70)
    print("VÃ‰RIFICATION DE LA QUALITÃ‰ DES DONNÃ‰ES")
    print("=" * 70)

    X = data.drop('Attrition', axis=1)

    issues = []

    # 1. Colonnes constantes
    constant_cols = [col for col in X.columns if X[col].nunique() == 1]
    if constant_cols:
        issues.append(f"âš ï¸  {len(constant_cols)} colonnes constantes trouvÃ©es: {constant_cols}")

    # 2. Duplicatas
    duplicates = data.duplicated().sum()
    if duplicates > 0:
        issues.append(f"âš ï¸  {duplicates} lignes dupliquÃ©es trouvÃ©es")

    # 3. Valeurs infinies
    inf_cols = [col for col in X.columns if np.isinf(X[col]).any()]
    if inf_cols:
        issues.append(f"âš ï¸  Valeurs infinies dans: {inf_cols}")

    # 4. Ã‰chelles trÃ¨s diffÃ©rentes (important pour SVM)
    scales = X.std()
    if scales.max() / scales.min() > 100:
        issues.append(f"âš ï¸  Ã‰chelles trÃ¨s diffÃ©rentes dÃ©tectÃ©es (ratio: {scales.max() / scales.min():.0f}:1)")
        issues.append("   â†’ Recommandation: UTILISER StandardScaler (OBLIGATOIRE pour SVM)")

    # Afficher les rÃ©sultats
    if issues:
        print("\nâŒ PROBLÃˆMES DÃ‰TECTÃ‰S:\n")
        for issue in issues:
            print(issue)
    else:
        print("\nâœ… Aucun problÃ¨me majeur dÃ©tectÃ©")

    return issues


def generate_data_report(filepath='data/prepared_data.csv'):
    """
    GÃ©nÃ¨re un rapport complet sur les donnÃ©es
    """
    data, correlations = analyze_data(filepath)
    check_data_quality(data)
    plot_top_correlations(data, n=10)

    print("\n" + "=" * 70)
    print("RECOMMANDATIONS POUR VOS MODÃˆLES")
    print("=" * 70)
    print("""
    1. ğŸ“Š PRÃ‰TRAITEMENT:
       âœ“ Utiliser StandardScaler (OBLIGATOIRE pour SVM)
       âœ“ Les donnÃ©es sont dÃ©jÃ  encodÃ©es (one-hot encoding)

    2. âš–ï¸  GESTION DU DÃ‰SÃ‰QUILIBRE:
       âœ“ Utiliser SMOTEENN ou SMOTE
       âœ“ Ajouter class_weight='balanced' dans les modÃ¨les

    3. ğŸš€ OPTIMISATION SVM:
       âœ“ Utiliser kernel='linear' (beaucoup plus rapide)
       âœ“ Normaliser les donnÃ©es AVANT
       âœ“ RÃ©duire C si trop lent (ex: C=0.1)

    4. ğŸ“ˆ RÃ‰GRESSION LOGISTIQUE:
       âœ“ Augmenter max_iter Ã  2000 minimum
       âœ“ Utiliser solver='lbfgs' et n_jobs=-1
       âœ“ InterprÃ©ter les coefficients avec prÃ©caution
    """)


if __name__ == "__main__":
    generate_data_report()
